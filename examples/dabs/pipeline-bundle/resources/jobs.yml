# resources/jobs.yml - Job definitions

resources:
  jobs:
    daily_etl_pipeline:
      name: "Daily ETL Pipeline - ${bundle.target}"
      
      # Schedule configuration
      schedule:
        quartz_cron_expression: "0 0 6 * * ?"
        timezone_id: "America/New_York"
        pause_status: UNPAUSED
      
      # Notifications
      email_notifications:
        on_failure:
          - ${var.notification_email}
        on_success: []
        no_alert_for_skipped_runs: true
      
      # Retry policy
      max_concurrent_runs: 1
      
      # Task definitions
      tasks:
        # ======== Bronze Layer ========
        - task_key: bronze_ingestion
          description: "Ingest raw data from source systems"
          notebook_task:
            notebook_path: ./src/notebooks/bronze_ingestion.py
            base_parameters:
              catalog: ${var.catalog}
              schema: bronze
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 2
            spark_conf:
              "spark.databricks.delta.preview.enabled": "true"
              "spark.sql.adaptive.enabled": "true"
            custom_tags:
              Layer: Bronze
              Pipeline: ETL
          timeout_seconds: 3600
          max_retries: 2
          min_retry_interval_millis: 60000
        
        # ======== Silver Layer ========
        - task_key: silver_transformation
          description: "Cleanse and validate data"
          depends_on:
            - task_key: bronze_ingestion
          notebook_task:
            notebook_path: ./src/notebooks/silver_transform.py
            base_parameters:
              catalog: ${var.catalog}
              source_schema: bronze
              target_schema: silver
          job_cluster_key: shared_cluster
          timeout_seconds: 7200
          max_retries: 1
        
        # ======== Gold Layer ========
        - task_key: gold_aggregation
          description: "Create business-ready aggregates"
          depends_on:
            - task_key: silver_transformation
          notebook_task:
            notebook_path: ./src/notebooks/gold_aggregate.py
            base_parameters:
              catalog: ${var.catalog}
              source_schema: silver
              target_schema: gold
          job_cluster_key: shared_cluster
          timeout_seconds: 3600
      
      # Shared job cluster for silver/gold
      job_clusters:
        - job_cluster_key: shared_cluster
          new_cluster:
            spark_version: "14.3.x-scala2.12"
            node_type_id: "Standard_DS4_v2"
            autoscale:
              min_workers: 2
              max_workers: 8
            spark_conf:
              "spark.databricks.delta.optimizeWrite.enabled": "true"
              "spark.databricks.delta.autoCompact.enabled": "true"
            custom_tags:
              Layer: SilverGold
              Pipeline: ETL
